{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Trajectory Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the links to papers and github repo with solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1508.00021.pdf  \n",
    "https://github.com/adbrebs/taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrival-clusters.pkl\t\t\t    taxi1.ipynb   train.csv\r\n",
      "arrivals.pkl\t\t\t\t    test2pickle   val_idx.pkl\r\n",
      "__MACOSX\t\t\t\t    test.csv\t  X_train.pkl\r\n",
      "metaData_taxistandsID_name_GPSlocation.csv  tmp\t\t  X_valid.pkl\r\n",
      "models\t\t\t\t\t    train2pickle\r\n"
     ]
    }
   ],
   "source": [
    "! ls /home/ubuntu/taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/taxi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# zip_ref = zipfile.ZipFile('train.csv.zip', 'r')\n",
    "# zip_ref.extractall(path)\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000589</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372637303620000596</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20000596</td>\n",
       "      <td>1372637303</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372636951620000320</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000320</td>\n",
       "      <td>1372636951</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372636854620000520</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000520</td>\n",
       "      <td>1372636854</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372637091620000337</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000337</td>\n",
       "      <td>1372637091</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n",
       "0  1372636858620000589         C          NaN           NaN  20000589   \n",
       "1  1372637303620000596         B          NaN           7.0  20000596   \n",
       "2  1372636951620000320         C          NaN           NaN  20000320   \n",
       "3  1372636854620000520         C          NaN           NaN  20000520   \n",
       "4  1372637091620000337         C          NaN           NaN  20000337   \n",
       "\n",
       "    TIMESTAMP DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858        A         False   \n",
       "1  1372637303        A         False   \n",
       "2  1372636951        A         False   \n",
       "3  1372636854        A         False   \n",
       "4  1372637091        A         False   \n",
       "\n",
       "                                            POLYLINE  \n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...  \n",
       "1  [[-8.639847,41.159826],[-8.640351,41.159871],[...  \n",
       "2  [[-8.612964,41.140359],[-8.613378,41.14035],[-...  \n",
       "3  [[-8.574678,41.151951],[-8.574705,41.151942],[...  \n",
       "4  [[-8.645994,41.18049],[-8.645949,41.180517],[-...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[[-8.618643,41.141412],[-8.618499,41.141376],[-8.620326,41.14251],[-8.622153,41.143815],[-8.623953,41.144373],[-8.62668,41.144778],[-8.627373,41.144697],[-8.630226,41.14521],[-8.632746,41.14692],[-8.631738,41.148225],[-8.629938,41.150385],[-8.62911,41.151213],[-8.629128,41.15124],[-8.628786,41.152203],[-8.628687,41.152374],[-8.628759,41.152518],[-8.630838,41.15268],[-8.632323,41.153022],[-8.631144,41.154489],[-8.630829,41.154507],[-8.630829,41.154516],[-8.630829,41.154498],[-8.630838,41.154489]]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train.POLYLINE[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[[-8.585676,41.148522],[-8.585712,41.148639],[-8.585685,41.148855],[-8.58573,41.148927],[-8.585982,41.148963],[-8.586396,41.148954],[-8.586072,41.14872],[-8.586324,41.147847],[-8.586999,41.14746],[-8.586576,41.147154],[-8.584884,41.146623]]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.POLYLINE[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20000542</td>\n",
       "      <td>1408039037</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.585676,41.148522],[-8.585712,41.148639],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>20000108</td>\n",
       "      <td>1408038611</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.610876,41.14557],[-8.610858,41.145579],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20000370</td>\n",
       "      <td>1408038568</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.585739,41.148558],[-8.58573,41.148828],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>20000492</td>\n",
       "      <td>1408039090</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.613963,41.141169],[-8.614125,41.141124],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20000621</td>\n",
       "      <td>1408039177</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.619903,41.148036],[-8.619894,41.148036]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID   TIMESTAMP DAY_TYPE  \\\n",
       "0      T1         B          NaN          15.0  20000542  1408039037        A   \n",
       "1      T2         B          NaN          57.0  20000108  1408038611        A   \n",
       "2      T3         B          NaN          15.0  20000370  1408038568        A   \n",
       "3      T4         B          NaN          53.0  20000492  1408039090        A   \n",
       "4      T5         B          NaN          18.0  20000621  1408039177        A   \n",
       "\n",
       "   MISSING_DATA                                           POLYLINE  \n",
       "0         False  [[-8.585676,41.148522],[-8.585712,41.148639],[...  \n",
       "1         False  [[-8.610876,41.14557],[-8.610858,41.145579],[-...  \n",
       "2         False  [[-8.585739,41.148558],[-8.58573,41.148828],[-...  \n",
       "3         False  [[-8.613963,41.141169],[-8.614125,41.141124],[...  \n",
       "4         False      [[-8.619903,41.148036],[-8.619894,41.148036]]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"metaData_taxistandsID_name_GPSlocation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Descricao</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Agra</td>\n",
       "      <td>41.1771457135</td>\n",
       "      <td>-8.609670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>41.15618964</td>\n",
       "      <td>-8.591064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aldoar</td>\n",
       "      <td>41.1705249231</td>\n",
       "      <td>-8.665876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alfândega</td>\n",
       "      <td>41.1437639911</td>\n",
       "      <td>-8.621803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Amial</td>\n",
       "      <td>41.1835097223</td>\n",
       "      <td>-8.612726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Descricao       Latitude  Longitude\n",
       "0   1       Agra  41.1771457135  -8.609670\n",
       "1   2    Alameda    41.15618964  -8.591064\n",
       "2   3     Aldoar  41.1705249231  -8.665876\n",
       "3   4  Alfândega  41.1437639911  -8.621803\n",
       "4   5      Amial  41.1835097223  -8.612726"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63, 4), (320, 9), (1710670, 9))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.shape, test.shape, train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. So we have 1.7 million training rides and only 320 test rides. Metadata is just having name of cities for lat/long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRIP_ID           int64\n",
       "CALL_TYPE        object\n",
       "ORIGIN_CALL     float64\n",
       "ORIGIN_STAND    float64\n",
       "TAXI_ID           int64\n",
       "TIMESTAMP         int64\n",
       "DAY_TYPE         object\n",
       "MISSING_DATA       bool\n",
       "POLYLINE         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winner's process to convert csv to hdf5 and extract features. I would prefer working on `pandas` for the same as it will allow me to look at how data looks. Replicating all steps present in `csv_to_hdf5.py` to make pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.copy()\n",
    "test2 = test.copy()\n",
    "meta2 = meta.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code proves that all the taxi ids in test set are also present in training set. We don't have any new taxi id in test data. But there are **20 origin calls** that are not in training data but are in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=0\n",
    "for l in test.TAXI_ID.unique():\n",
    "    if l not in train.TAXI_ID.unique():\n",
    "        s = s+1\n",
    "p=0\n",
    "g = []\n",
    "for l in test.ORIGIN_CALL.unique():\n",
    "    if l not in train.ORIGIN_CALL.unique():\n",
    "        p = p+1\n",
    "        g.append(l)\n",
    "s,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g` as saved above has list of origin_calls that are in test but not in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-calculated below on train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mean = 41.15731\n",
    "lat_std = 0.074120656\n",
    "long_mean = -8.6161413\n",
    "long_std = 0.057200309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a new feature with only first 5 and last 5 coordinates. In cases where we don't have enough coordinates, we do `edge` padding (i.e. just adding first and last values in leftmost and rightmost sides respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_stop_inputs(k, train, test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function used to take first k and last k coordinates from given polyline.\n",
    "    For train - if total coordinates < 2 then return none, if > 2*k then return first k and last k\n",
    "    and if > 2 but < 2*k then do edge padding and return.\n",
    "    \n",
    "    \"\"\"\n",
    "    result_train = []\n",
    "    result_test = []\n",
    "    \n",
    "    for l in train[['LONGITUDE','LATITUDE']].iterrows():\n",
    "        if len(l[1][0]) < 2 or len(l[1][1]) < 2:\n",
    "            result_train.append(np.nan)\n",
    "        elif len(l[1][0][:-1]) >= 2*k:\n",
    "            result_train.append(np.concatenate([l[1][0][0:k],l[1][0][-k:],\n",
    "                                          l[1][1][0:k],l[1][1][-k:]]).flatten())\n",
    "        else:\n",
    "            l1 = np.lib.pad(l[1][0][:-1], (0,4*k-len(l[1][0][:-1])), mode='edge')\n",
    "            l2 = np.lib.pad(l[1][1][:-1], (0,4*k-len(l[1][1][:-1])), mode='edge')\n",
    "            result_train.append(np.concatenate([l1[0:k],l1[-k:],l2[0:k],l2[-k:]]).flatten())\n",
    "\n",
    "    for l in test[['LONGITUDE','LATITUDE']].iterrows(): \n",
    "        if len(l[1][0]) < 1 or len(l[1][1]) < 1:\n",
    "            result_test.append(np.nan)\n",
    "        elif len(l[1][0]) >= 2*k:\n",
    "            result_test.append(np.concatenate([l[1][0][0:k],l[1][0][-k:],l[1][1][0:k],l[1][1][-k:]]).flatten())\n",
    "        else:\n",
    "            l1 = np.lib.pad(l[1][0], (0,4*k-len(l[1][0])), mode='edge')\n",
    "            l2 = np.lib.pad(l[1][1], (0,4*k-len(l[1][1])), mode='edge')\n",
    "            result_test.append(np.concatenate([l1[0:k],l1[-k:],l2[0:k],l2[-k:]]).flatten())\n",
    "    \n",
    "    return pd.Series(result_train), pd.Series(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def feature_ext(train, test):   \n",
    "    \"\"\"\n",
    "    Function used to extract model features.\n",
    "    • extract lat/long from polyline\n",
    "    • lat/long normalized using pre-calculated mean and std\n",
    "    • origin_call and taxi_id converted to continuous numbers to get embeddings later\n",
    "    • categories convert to cat codes\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # origin_call -- train\n",
    "    train.ORIGIN_CALL.fillna(value = -1, inplace=True)\n",
    "    id_uniq = train.ORIGIN_CALL.unique()\n",
    "    \n",
    "    origin_call_dict = {o:i for i,o in enumerate(id_uniq)}\n",
    "\n",
    "    train['ORIGIN_CALL'] = train.ORIGIN_CALL.apply(lambda x: 0 if x == -1 \n",
    "                                                   or x == '' else origin_call_dict[x])\n",
    "    \n",
    "    \n",
    "    # origin_call --test (g list of ids in test not in train)\n",
    "    test.ORIGIN_CALL.fillna(value = -1, inplace=True)\n",
    "\n",
    "    test['ORIGIN_CALL'] = test.ORIGIN_CALL.apply(lambda x: 0 if x == -1 or x == '' \n",
    "                                              else (-2 if x in g[1:] else origin_call_dict[x]))\n",
    "    \n",
    "    \n",
    "    # origin_stand\n",
    "    train['ORIGIN_STAND']= pd.Series([0 if pd.isnull(x) or x=='' else int(x) for x in train[\"ORIGIN_STAND\"]])\n",
    "    test['ORIGIN_STAND']= pd.Series([0 if pd.isnull(x) or x=='' else int(x) for x in test[\"ORIGIN_STAND\"]])\n",
    "    \n",
    "    # taxi_id\n",
    "    id_uniq = train.TAXI_ID.unique()\n",
    "    taxi_id_dict = {o:i for i,o in enumerate(id_uniq)}\n",
    "    train['TAXI_ID'] = train.TAXI_ID.apply(lambda x: taxi_id_dict[x])\n",
    "    test['TAXI_ID'] = test.TAXI_ID.apply(lambda x: taxi_id_dict[x])\n",
    "\n",
    "    # day_type and call_type\n",
    "    # want 0 for A, 1 for B and 2 for C\n",
    "    train['DAY_TYPE'] = pd.Series([ord(x[0]) - ord('A') for x in train['DAY_TYPE']])\n",
    "    train['CALL_TYPE'] = pd.Series([ord(x[0]) - ord('A') for x in train['CALL_TYPE']])\n",
    "    test['DAY_TYPE'] = pd.Series([ord(x[0]) - ord('A') for x in test['DAY_TYPE']])\n",
    "    test['CALL_TYPE'] = pd.Series([ord(x[0]) - ord('A') for x in test['CALL_TYPE']])\n",
    "    \n",
    "    \n",
    "    # polyline\n",
    "    polyline1 = pd.Series([ast.literal_eval(x) for x in train['POLYLINE']])\n",
    "    polyline2 = pd.Series([ast.literal_eval(x) for x in test['POLYLINE']])\n",
    "    \n",
    "    # latitude and longitude\n",
    "    train['LATITUDE'] = pd.Series([np.array([point[1] for point in poly],dtype=np.float32) for poly in polyline1])\n",
    "    train['LONGITUDE'] = pd.Series([np.array([point[0] for point in poly],dtype=np.float32) for poly in polyline1])\n",
    "    \n",
    "    test['LATITUDE'] = pd.Series([np.array([point[1] for point in poly],dtype=np.float32) for poly in polyline2])\n",
    "    test['LONGITUDE'] = pd.Series([np.array([point[0] for point in poly],dtype=np.float32) for poly in polyline2])\n",
    "    \n",
    "    # target variable i.e. last coordinates\n",
    "    train['TARGET'] = pd.Series([[l[1][0][-1], l[1][1][-1]] if len(l[1][0]) > 1 \n",
    "                                else np.nan for l in train[['LONGITUDE','LATITUDE']].iterrows()])\n",
    "\n",
    "    # normalized lat long. We need normalized data for neural nets and this is like cont. variable (not cat)\n",
    "    train['LATITUDE'] = pd.Series([(t-lat_mean)/lat_std for t in train['LATITUDE']])\n",
    "    test['LATITUDE'] = pd.Series([(t-lat_mean)/lat_std for t in test['LATITUDE']])\n",
    "    \n",
    "    train['LONGITUDE'] = pd.Series([(t-long_mean)/long_std for t in train['LONGITUDE']])\n",
    "    test['LONGITUDE'] = pd.Series([(t-long_mean)/long_std for t in test['LONGITUDE']])\n",
    "\n",
    "    # first 5 and last 5 features\n",
    "    train['COORD_FEATURES'], test['COORD_FEATURES'] = start_stop_inputs(5, train, test)\n",
    "    \n",
    "    # day of week variable\n",
    "    train['DAY_OF_WEEK'] = pd.Series([datetime.datetime.fromtimestamp(t).weekday() for t in train['TIMESTAMP']])\n",
    "    test['DAY_OF_WEEK'] = pd.Series([datetime.datetime.fromtimestamp(t).weekday() for t in test['TIMESTAMP']])\n",
    "\n",
    "    # hour of day\n",
    "    train['HOUR'] = pd.Series([datetime.datetime.fromtimestamp(t).hour for t in train['TIMESTAMP']])\n",
    "    test['HOUR'] = pd.Series([datetime.datetime.fromtimestamp(t).hour for t in test['TIMESTAMP']])\n",
    "\n",
    "    # week of year\n",
    "    train['WEEK_OF_YEAR'] = pd.Series([datetime.datetime.fromtimestamp(t).isocalendar()[1] for t in train['TIMESTAMP']])\n",
    "    test['WEEK_OF_YEAR'] = pd.Series([datetime.datetime.fromtimestamp(t).isocalendar()[1] for t in test['TIMESTAMP']])\n",
    "\n",
    "        \n",
    "    #data = data.dropna()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, test2 = train.copy(), test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, test2 = feature_ext(train2, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>COORD_FEATURES</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>WEEK_OF_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "      <td>[-0.21451, -0.214974, -0.199688, -0.182087, -0...</td>\n",
       "      <td>[-0.0437321, -0.0412145, -0.0731591, -0.105104...</td>\n",
       "      <td>[-8.63084, 41.1545]</td>\n",
       "      <td>[-0.0437321, -0.0412145, -0.0731591, -0.105104...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372637303620000596</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1372637303</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n",
       "      <td>[0.0339161, 0.0345337, 0.0378275, 0.0429227, 0...</td>\n",
       "      <td>[-0.414429, -0.423249, -0.455494, -0.494991, -...</td>\n",
       "      <td>[-8.66574, 41.1707]</td>\n",
       "      <td>[-0.414429, -0.423249, -0.455494, -0.494991, -...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID  CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND  TAXI_ID  \\\n",
       "0  1372636858620000589          2            0             0        0   \n",
       "1  1372637303620000596          1            0             7        1   \n",
       "\n",
       "    TIMESTAMP  DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858         0         False   \n",
       "1  1372637303         0         False   \n",
       "\n",
       "                                            POLYLINE  \\\n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...   \n",
       "1  [[-8.639847,41.159826],[-8.640351,41.159871],[...   \n",
       "\n",
       "                                            LATITUDE  \\\n",
       "0  [-0.21451, -0.214974, -0.199688, -0.182087, -0...   \n",
       "1  [0.0339161, 0.0345337, 0.0378275, 0.0429227, 0...   \n",
       "\n",
       "                                           LONGITUDE               TARGET  \\\n",
       "0  [-0.0437321, -0.0412145, -0.0731591, -0.105104...  [-8.63084, 41.1545]   \n",
       "1  [-0.414429, -0.423249, -0.455494, -0.494991, -...  [-8.66574, 41.1707]   \n",
       "\n",
       "                                      COORD_FEATURES  DAY_OF_WEEK  HOUR  \\\n",
       "0  [-0.0437321, -0.0412145, -0.0731591, -0.105104...            0     0   \n",
       "1  [-0.414429, -0.423249, -0.455494, -0.494991, -...            0     0   \n",
       "\n",
       "   WEEK_OF_YEAR  \n",
       "0            27  \n",
       "1            27  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>COORD_FEATURES</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>WEEK_OF_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>1408039037</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.585676,41.148522],[-8.585712,41.148639],[...</td>\n",
       "      <td>[-0.118578, -0.116982, -0.1141, -0.113122, -0....</td>\n",
       "      <td>[0.532604, 0.531971, 0.532454, 0.531671, 0.527...</td>\n",
       "      <td>[0.532604, 0.531971, 0.532454, 0.531671, 0.527...</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>306</td>\n",
       "      <td>1408038611</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.610876,41.14557],[-8.610858,41.145579],[-...</td>\n",
       "      <td>[-0.158413, -0.158258, -0.155736, -0.150024, -...</td>\n",
       "      <td>[0.0920491, 0.0923659, 0.0915823, 0.0996017, 0...</td>\n",
       "      <td>[0.0920491, 0.0923659, 0.0915823, 0.0996017, 0...</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TRIP_ID  CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND  TAXI_ID   TIMESTAMP  \\\n",
       "0      T1          1            0            15      150  1408039037   \n",
       "1      T2          1            0            57      306  1408038611   \n",
       "\n",
       "   DAY_TYPE  MISSING_DATA                                           POLYLINE  \\\n",
       "0         0         False  [[-8.585676,41.148522],[-8.585712,41.148639],[...   \n",
       "1         0         False  [[-8.610876,41.14557],[-8.610858,41.145579],[-...   \n",
       "\n",
       "                                            LATITUDE  \\\n",
       "0  [-0.118578, -0.116982, -0.1141, -0.113122, -0....   \n",
       "1  [-0.158413, -0.158258, -0.155736, -0.150024, -...   \n",
       "\n",
       "                                           LONGITUDE  \\\n",
       "0  [0.532604, 0.531971, 0.532454, 0.531671, 0.527...   \n",
       "1  [0.0920491, 0.0923659, 0.0915823, 0.0996017, 0...   \n",
       "\n",
       "                                      COORD_FEATURES  DAY_OF_WEEK  HOUR  \\\n",
       "0  [0.532604, 0.531971, 0.532454, 0.531671, 0.527...            3    17   \n",
       "1  [0.0920491, 0.0923659, 0.0915823, 0.0996017, 0...            3    17   \n",
       "\n",
       "   WEEK_OF_YEAR  \n",
       "0            33  \n",
       "1            33  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving transformed files as `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.to_pickle(path+'train2pickle')\n",
    "test2.to_pickle(path+'test2pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading train and test. **Can directly run this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.read_pickle('train2pickle')\n",
    "test2 = pd.read_pickle('test2pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Meanshift clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from itertools import cycle\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.17067, -8.66574)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how to index and subset lat and long (lat is in 1 and long in 0th)\n",
    "train2.TARGET[:10][1][1], train2.TARGET[:10][1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have polyline data for all the training data. For those, we have `nan` in `TARGET` variable and those won't contribute in centroid calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating arrival point list\n",
      "Doing clustering\n",
      "0.0215466177411\n",
      "Clusters shape:  (3422, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating arrival point list\")\n",
    "dests = []\n",
    "\n",
    "# for l in train2[['LONGITUDE','LATITUDE']].iterrows():\n",
    "#     if len(l[1][0]) == 0 or len(l[1][1]) == 0: continue\n",
    "#     dests.append((l[1][1][-1],  l[1][0][-1])) # latitude, longitude\n",
    "\n",
    "target = train2.TARGET.fillna(0)    \n",
    "for l in target:\n",
    "    if l == 0 : continue\n",
    "    dests.append((l[1],  l[0])) # latitude, longitude\n",
    "    \n",
    "pts = np.array(dests)\n",
    "\n",
    "with open(os.path.join(path, \"arrivals.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pts, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Doing clustering\")\n",
    "bw = estimate_bandwidth(pts, quantile=.1, n_samples=1000)\n",
    "print(bw)\n",
    "bw = 0.001 \n",
    "\n",
    "ms = MeanShift(bandwidth=bw, bin_seeding=True, min_bin_freq=5)\n",
    "ms.fit(pts)\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "print(\"Clusters shape: \", cluster_centers.shape)\n",
    "\n",
    "with open(os.path.join(path, \"arrival-clusters.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(cluster_centers, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried above code for --  \n",
    "1. scaled coordinates. It doesn't give even close number of clusters. got **22644 clusters**. Not quite right. \n",
    "2. unscaled coordinates. Got **3422 clusters**. Winner had 3392 clusters. CLOSE and will keep it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Making Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As given in winner's solution, we are taking validation set as last year's data with same timestamp as of test data timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuts of the test set minus 1 year\n",
    "cuts = [\n",
    "    1376503200, # 2013-08-14 18:00\n",
    "    1380616200, # 2013-10-01 08:30\n",
    "    1381167900, # 2013-10-07 17:45\n",
    "    1383364800, # 2013-11-02 04:00\n",
    "    1387722600  # 2013-12-22 14:30\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indices = []\n",
    "index = 0\n",
    "for index, row in train2.iterrows():\n",
    "    time = row['TIMESTAMP']\n",
    "    latitude = row['LATITUDE']\n",
    "    for ts in cuts:\n",
    "        if time <= ts and time + 15 * (len(latitude) - 1) >= ts:\n",
    "            val_indices.append(index)\n",
    "            break\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving and reloading validation indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"val_idx.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(val_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"val_idx.pkl\"), \"rb\") as f:\n",
    "    val_indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = train2.iloc[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have right timestamps in validation set or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{datetime.date(2013, 8, 14),\n",
       " datetime.date(2013, 10, 1),\n",
       " datetime.date(2013, 10, 7),\n",
       " datetime.date(2013, 11, 2),\n",
       " datetime.date(2013, 12, 22)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt = []\n",
    "for d in X_valid['TIMESTAMP']:\n",
    "    lt.append(datetime.date.fromtimestamp(d))\n",
    "    \n",
    "set(lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup. above dates match with our dates of test set. Just 1 previous year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train2.drop(train2.index[[val_indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 1710366)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_valid), len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set length is similar as test set length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"arrival-clusters.pkl\"), \"rb\") as f:\n",
    "    cluster_centers2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3422"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_centers2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving `X_train` and `X_valid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(path + 'X_train.pkl')\n",
    "X_valid.to_pickle(path + 'X_valid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading validation and training files. Next time can directly read from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('X_train.pkl')\n",
    "X_valid = pd.read_pickle('X_valid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer is basically telling probability of observation being in ith cluster. So it will have as many outputs as number of clusters. (3422). It includes training for `probabilities`  \n",
    "\n",
    "Now sum of all the `probabilities` should be 1. Therefore we will use `softmax` over a trained linear layer\n",
    "\n",
    "\n",
    "On these 3422 output from hidden, we take weighted avg which will just be sum of multiplied probability into cluster center (i.e. sum of 3422 elements). It includes no traning, just using previous probabilities and calculating weighted avg.   \n",
    "\n",
    "Think of this operation as a linear layer with `weights = centers (i.e. fixed) and input = probabilities`\n",
    "\n",
    "\n",
    "\n",
    "Layers to add:  \n",
    "1. Pass input data to a **dense layer** which has 500 outputs and those **500 to relu**. \n",
    "1. Pass coming data to a **linear layer** which has 3422 outputs\n",
    "2. These 3422 outputs passed to a **softmax** (which will give p)\n",
    "3. These 3422 p's will go to **linear layer** which **does not** update weights.  just multiply p and c . \n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing cardinality for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ORIGIN_CALL', 'TAXI_ID', 'ORIGIN_STAND', 'HOUR', 'DAY_OF_WEEK', 'WEEK_OF_YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGIN_CALL -- Possible values 57106 || Embedding size 10\n",
      "TAXI_ID -- Possible values 448 || Embedding size 10\n",
      "ORIGIN_STAND -- Possible values 64 || Embedding size 10\n",
      "HOUR -- Possible values 24 || Embedding size 10\n",
      "DAY_OF_WEEK -- Possible values 7 || Embedding size 10\n",
      "WEEK_OF_YEAR -- Possible values 52 || Embedding size 10\n"
     ]
    }
   ],
   "source": [
    "for i in cols:\n",
    "    print(i, '--', 'Possible values', len(train2[i].unique()) ,'||', 'Embedding size', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They took embedding of size 10 for all the above features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57106, 10), (448, 10), (64, 10), (24, 10), (7, 10), (52, 10)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs = []\n",
    "for i in cols:\n",
    "    emb_szs.append((len(train2[i].unique()), 10))\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['ORIGIN_CALL', 'TAXI_ID', 'ORIGIN_STAND', 'HOUR', 'DAY_OF_WEEK', 'WEEK_OF_YEAR']\n",
    "#cont_vars = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>COORD_FEATURES</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>WEEK_OF_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "      <td>[-0.21451, -0.214974, -0.199688, -0.182087, -0...</td>\n",
       "      <td>[-0.0437321, -0.0412145, -0.0731591, -0.105104...</td>\n",
       "      <td>[-8.63084, 41.1545]</td>\n",
       "      <td>[-0.0437321, -0.0412145, -0.0731591, -0.105104...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID  CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND  TAXI_ID  \\\n",
       "0  1372636858620000589          2            0             0        0   \n",
       "\n",
       "    TIMESTAMP  DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858         0         False   \n",
       "\n",
       "                                            POLYLINE  \\\n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...   \n",
       "\n",
       "                                            LATITUDE  \\\n",
       "0  [-0.21451, -0.214974, -0.199688, -0.182087, -0...   \n",
       "\n",
       "                                           LONGITUDE               TARGET  \\\n",
       "0  [-0.0437321, -0.0412145, -0.0731591, -0.105104...  [-8.63084, 41.1545]   \n",
       "\n",
       "                                      COORD_FEATURES  DAY_OF_WEEK  HOUR  \\\n",
       "0  [-0.0437321, -0.0412145, -0.0731591, -0.105104...            0     0   \n",
       "\n",
       "   WEEK_OF_YEAR  \n",
       "0            27  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NA with 0s\n",
    "# k = 5\n",
    "\n",
    "X_train.COORD_FEATURES.fillna(0, inplace = True)\n",
    "X_valid.COORD_FEATURES.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    \n",
    "    cols = ['ORIGIN_CALL', 'TAXI_ID', 'ORIGIN_STAND', 'HOUR', 'DAY_OF_WEEK', 'WEEK_OF_YEAR','MISSING_DATA']\n",
    "    \n",
    "    df_feat = df[cols]\n",
    "    for i in range(10):\n",
    "        df_feat[f'long_{i}'] = df.COORD_FEATURES.apply(lambda x: 0 if type(x) == int else x[i]) # creating 10 cols for long\n",
    "        df_feat[f'lat_{i}'] = df.COORD_FEATURES.apply(lambda x: 0 if type(x) == int else x[i+10]) # 10 cols for lat\n",
    "\n",
    "    df_target = df['TARGET']\n",
    "               \n",
    "    return df_feat, df_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.COORD_FEATURES.fillna(0, inplace = True)\n",
    "df, y = get_features(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat, train_target = get_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_feat, valid_target = get_features(X_valid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining nn for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centers of cluster lats and longs. Needed for last nn layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_lats = []\n",
    "\n",
    "for i in cluster_centers2:\n",
    "    cluster_lats.append(i[0])\n",
    "    \n",
    "cluster_longs = []\n",
    "\n",
    "for i in cluster_centers2:\n",
    "    cluster_longs.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_lats = np.array(cluster_lats)[:,None]\n",
    "cluster_longs = np.array(cluster_longs)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed input nn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customizing `fast.ai` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.dataset import *\n",
    "from fastai.learner import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After loading data into dataloader, writing my own training loop (tried `get_learner` from fastai. doesn't work for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class taxi_dataset_from_df():\n",
    "    def __init__(self, df, cats, y):\n",
    "        #n = len(cats[0]) if cats else len(conts[0])\n",
    "        \n",
    "        self.df_cats = df[cats]\n",
    "        self.df_conts = df.drop(cats, axis=1)\n",
    "        \n",
    "        lat = y.fillna(0).apply(lambda x: 0 if type(x) == int else x[1])\n",
    "        long = y.fillna(0).apply(lambda x: 0 if type(x) == int else x[0])\n",
    "        \n",
    "        target = pd.DataFrame({'long': long.astype(np.float32),'lat' : lat.astype(np.float32)})\n",
    "        \n",
    "        cat_cols = [c.values for n,c in self.df_cats.items()]\n",
    "        cont_cols = [c.values for n,c in self.df_conts.items()]  \n",
    "        target_cols = [c.values for n,c in target.items()]  \n",
    "        \n",
    "        self.cats = np.stack(cat_cols, 1).astype(np.int64)\n",
    "        self.conts = np.stack(cont_cols, 1).astype(np.float32) \n",
    "        self.target = np.stack(target_cols, 1).astype(np.float32) \n",
    "\n",
    "    def __len__(self): return len(self.df_cats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.target[idx]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = taxi_dataset_from_df(train_feat, cat_vars, train_target)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = taxi_dataset_from_df(valid_feat, cat_vars, valid_target)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see what my dataset is returning. It is a list with 3 elements. cat df, cont df and y.\n",
    "\n",
    "**NOTICE that lat is in 0 here and long is in 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0,  0,  0,  0,  0, 27],\n",
       "        [ 0,  1,  7,  0,  0, 27],\n",
       "        [ 0,  2,  0,  0,  0, 27]]),\n",
       " array([[ 0.     , -0.04373, -0.21451, -0.04121, -0.21497, -0.07316, -0.19969, -0.1051 , -0.18209, -0.13656,\n",
       "         -0.17452, -0.26228, -0.03808, -0.25677, -0.03783, -0.25677, -0.03772, -0.25677, -0.03793, -0.25694,\n",
       "         -0.03808],\n",
       "        [ 0.     , -0.41443,  0.03392, -0.42325,  0.03453, -0.45549,  0.03783, -0.49499,  0.04292, -0.53811,\n",
       "          0.0491 , -0.93476,  0.14369, -0.90834,  0.15754, -0.88284,  0.17128, -0.86757,  0.17977, -0.86711,\n",
       "          0.18023],\n",
       "        [ 0.     ,  0.05555, -0.22872,  0.04832, -0.22882,  0.03368, -0.2298 ,  0.02393, -0.22856,  0.0041 ,\n",
       "         -0.22748, -0.13893, -0.17334, -0.09062, -0.18765, -0.05774, -0.2071 , -0.02139, -0.22192,  0.003  ,\n",
       "         -0.2264 ]], dtype=float32),\n",
       " array([[ 41.15449,  -8.63084],\n",
       "        [ 41.17067,  -8.66574],\n",
       "        [ 41.14053,  -8.61597]], dtype=float32)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. So let's think on how will I use input of these for in dataloader and that into my `Net`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They used `equirectangular distance` as loss function which performed better than original kaggle metric (`Haversine distance`). Actually equirectangular distance is just a simpler approximation to haversine distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will keep long in 0 and lat in 1 as there in target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a predicted, b true\n",
    "# data loader is in such a way that true y should be indexed from floatTensor\n",
    "\n",
    "class equiRectLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(equiRectLoss, self).__init__()\n",
    "        self.rearth = 6371\n",
    "        self.deg2rad = 3.141592653589793 / 180.\n",
    "\n",
    "    def forward(self, a,b):\n",
    "#         lat1 = a.fillna(0).apply(lambda x: 0 if type(x) == int else x[1]) * deg2rad\n",
    "#         lon1 = a.fillna(0).apply(lambda x: 0 if type(x) == int else x[0]) * deg2rad\n",
    "#         lat2 = b.fillna(0).apply(lambda x: 0 if type(x) == int else x[1]) * deg2rad\n",
    "#         lon2 = b.fillna(0).apply(lambda x: 0 if type(x) == int else x[0]) * deg2rad    \n",
    "        lat1 = a[:,0]\n",
    "        lon1 = a[:,1]\n",
    "        lat2 = b[:,0]\n",
    "        lon2 = b[:,1]   \n",
    "        \n",
    "        x = (lon2-lon1) * torch.cos((lat1+lat2)/2)\n",
    "        y = (lat2-lat1)\n",
    "\n",
    "        loss = torch.mean((torch.sqrt(x**2 + y**2) * self.rearth),dim=0)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_init(x):\n",
    "    x = x.weight.data\n",
    "    sc = 2/(x.size(1)+1)\n",
    "    x.uniform_(-sc,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they didn't do batchnormm in paper            \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, \n",
    "                 y_range=None, use_bn=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n",
    "        for emb in self.embs: emb_init(emb)\n",
    "        #n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "        n_emb = 60\n",
    "        \n",
    "        # 6 cat vars . therefore 60 embeddings (10 each)\n",
    "        self.lin1 = nn.Linear(n_emb + n_cont, 500) # 1st linear layer -- no. features x 500\n",
    "        self.relu = nn.ReLU()   # 2nd relu layer (output = 500 neurons)\n",
    "        self.lin2 = nn.Linear(500, 3422)  # 3rd linear layer. 500x3422\n",
    "        self.soft = nn.Softmax() # 4th softmax layer. 3422 probabilities (will sum to 1)\n",
    "         \n",
    "        for o in [self.lin1, self.lin2] : kaiming_normal(o.weight.data) #weight initialization for lin layers\n",
    "            \n",
    "        self.bn = nn.BatchNorm1d(n_cont) \n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x2 = self.bn(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        \n",
    "        out = self.lin1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.soft()\n",
    "        outlat = out*cluster_lats  # out is n*3422 and cluster_lats is 3422*1\n",
    "        outlong = out*cluster_longs\n",
    "        \n",
    "        #target = pd.DataFrame({'long': outlong.astype(np.float32),'lat' : outlat.astype(np.float32)})\n",
    "        #target_cols = [c.values for n,c in target.items()]  \n",
    "        target_out = np.stack((outlat, outlong), axis=1)\n",
    "\n",
    "        return target_out # nx2 tensor where n is no. of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)\n",
    "a,b,c = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 6]), torch.Size([200, 21]), torch.Size([200, 2]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-cc850c53bc11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_szs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0memb_szs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cont\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-4f1dcc4e64d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-4f1dcc4e64d0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)"
     ]
    }
   ],
   "source": [
    "net2 = Net(emb_szs= emb_szs, n_cont= len(df.columns)-len(cat_vars), y_range=y)\n",
    "net2(V(a),V(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, y):\n",
    "    y_pred = to_np(net2(V(x)))\n",
    "    return np.sum(y_pred.argmax(axis=1) == to_np(y))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-ad9bceed1dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Forward pass: compute predicted y and loss by passing x to the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mxcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxcont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxcont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-4f1dcc4e64d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-4f1dcc4e64d0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)"
     ]
    }
   ],
   "source": [
    "net2 = Net(emb_szs= emb_szs, n_cont= len(df.columns)-len(cat_vars), y_range=y)\n",
    "#net2 = to_gpu(net2)\n",
    "loss=equiRectLoss\n",
    "learning_rate = 1e-2\n",
    "optimizer=optim.SGD(net2.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "for epoch in range(3):\n",
    "    losses=[]\n",
    "    #dl = iter(train_loader)\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "    #for t in range(len(dl)):\n",
    "        # Forward pass: compute predicted y and loss by passing x to the model.\n",
    "        xcat, xcont, yt = data      \n",
    "        y_pred = net2(V(xcat), V(xcont))\n",
    "        l = loss(y_pred, V(yt))\n",
    "        losses.append(l)\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable weights of the model)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        l.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    val_dl = iter(valid_loader)\n",
    "    val_scores = [score(*next(val_dl)) for i in range(len(val_dl))]\n",
    "    print(np.mean(val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
