{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Trajectory Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the links to papers and github repo with solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1508.00021.pdf  \n",
    "https://github.com/adbrebs/taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrival-clusters.pkl\t\t\t    taxi1.ipynb   train.csv\r\n",
      "arrivals.pkl\t\t\t\t    test2pickle   val_idx.pkl\r\n",
      "__MACOSX\t\t\t\t    test.csv\t  X_train.pkl\r\n",
      "metaData_taxistandsID_name_GPSlocation.csv  tmp\t\t  X_valid.pkl\r\n",
      "models\t\t\t\t\t    train2pickle\r\n"
     ]
    }
   ],
   "source": [
    "! ls /home/ubuntu/taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.structured import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/taxi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# zip_ref = zipfile.ZipFile('train.csv.zip', 'r')\n",
    "# zip_ref.extractall(path)\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000589</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372637303620000596</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20000596</td>\n",
       "      <td>1372637303</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372636951620000320</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000320</td>\n",
       "      <td>1372636951</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372636854620000520</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000520</td>\n",
       "      <td>1372636854</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372637091620000337</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000337</td>\n",
       "      <td>1372637091</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n",
       "0  1372636858620000589         C          NaN           NaN  20000589   \n",
       "1  1372637303620000596         B          NaN           7.0  20000596   \n",
       "2  1372636951620000320         C          NaN           NaN  20000320   \n",
       "3  1372636854620000520         C          NaN           NaN  20000520   \n",
       "4  1372637091620000337         C          NaN           NaN  20000337   \n",
       "\n",
       "    TIMESTAMP DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858        A         False   \n",
       "1  1372637303        A         False   \n",
       "2  1372636951        A         False   \n",
       "3  1372636854        A         False   \n",
       "4  1372637091        A         False   \n",
       "\n",
       "                                            POLYLINE  \n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...  \n",
       "1  [[-8.639847,41.159826],[-8.640351,41.159871],[...  \n",
       "2  [[-8.612964,41.140359],[-8.613378,41.14035],[-...  \n",
       "3  [[-8.574678,41.151951],[-8.574705,41.151942],[...  \n",
       "4  [[-8.645994,41.18049],[-8.645949,41.180517],[-...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[[-8.618643,41.141412],[-8.618499,41.141376],[-8.620326,41.14251],[-8.622153,41.143815],[-8.623953,41.144373],[-8.62668,41.144778],[-8.627373,41.144697],[-8.630226,41.14521],[-8.632746,41.14692],[-8.631738,41.148225],[-8.629938,41.150385],[-8.62911,41.151213],[-8.629128,41.15124],[-8.628786,41.152203],[-8.628687,41.152374],[-8.628759,41.152518],[-8.630838,41.15268],[-8.632323,41.153022],[-8.631144,41.154489],[-8.630829,41.154507],[-8.630829,41.154516],[-8.630829,41.154498],[-8.630838,41.154489]]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train.POLYLINE[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[[-8.585676,41.148522],[-8.585712,41.148639],[-8.585685,41.148855],[-8.58573,41.148927],[-8.585982,41.148963],[-8.586396,41.148954],[-8.586072,41.14872],[-8.586324,41.147847],[-8.586999,41.14746],[-8.586576,41.147154],[-8.584884,41.146623]]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.POLYLINE[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20000542</td>\n",
       "      <td>1408039037</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.585676,41.148522],[-8.585712,41.148639],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>20000108</td>\n",
       "      <td>1408038611</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.610876,41.14557],[-8.610858,41.145579],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20000370</td>\n",
       "      <td>1408038568</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.585739,41.148558],[-8.58573,41.148828],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>20000492</td>\n",
       "      <td>1408039090</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.613963,41.141169],[-8.614125,41.141124],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20000621</td>\n",
       "      <td>1408039177</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.619903,41.148036],[-8.619894,41.148036]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID   TIMESTAMP DAY_TYPE  \\\n",
       "0      T1         B          NaN          15.0  20000542  1408039037        A   \n",
       "1      T2         B          NaN          57.0  20000108  1408038611        A   \n",
       "2      T3         B          NaN          15.0  20000370  1408038568        A   \n",
       "3      T4         B          NaN          53.0  20000492  1408039090        A   \n",
       "4      T5         B          NaN          18.0  20000621  1408039177        A   \n",
       "\n",
       "   MISSING_DATA                                           POLYLINE  \n",
       "0         False  [[-8.585676,41.148522],[-8.585712,41.148639],[...  \n",
       "1         False  [[-8.610876,41.14557],[-8.610858,41.145579],[-...  \n",
       "2         False  [[-8.585739,41.148558],[-8.58573,41.148828],[-...  \n",
       "3         False  [[-8.613963,41.141169],[-8.614125,41.141124],[...  \n",
       "4         False      [[-8.619903,41.148036],[-8.619894,41.148036]]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"metaData_taxistandsID_name_GPSlocation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Descricao</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Agra</td>\n",
       "      <td>41.1771457135</td>\n",
       "      <td>-8.609670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>41.15618964</td>\n",
       "      <td>-8.591064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aldoar</td>\n",
       "      <td>41.1705249231</td>\n",
       "      <td>-8.665876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alfândega</td>\n",
       "      <td>41.1437639911</td>\n",
       "      <td>-8.621803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Amial</td>\n",
       "      <td>41.1835097223</td>\n",
       "      <td>-8.612726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Descricao       Latitude  Longitude\n",
       "0   1       Agra  41.1771457135  -8.609670\n",
       "1   2    Alameda    41.15618964  -8.591064\n",
       "2   3     Aldoar  41.1705249231  -8.665876\n",
       "3   4  Alfândega  41.1437639911  -8.621803\n",
       "4   5      Amial  41.1835097223  -8.612726"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63, 4), (320, 9), (1710670, 9))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.shape, test.shape, train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. So we have 1.7 million training rides and only 320 test rides. Metadata is just having name of cities for lat/long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRIP_ID           int64\n",
       "CALL_TYPE        object\n",
       "ORIGIN_CALL     float64\n",
       "ORIGIN_STAND    float64\n",
       "TAXI_ID           int64\n",
       "TIMESTAMP         int64\n",
       "DAY_TYPE         object\n",
       "MISSING_DATA       bool\n",
       "POLYLINE         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winner's process to convert csv to hdf5 and extract features. I would prefer working on `pandas` for the same as it will allow me to look at how data looks. Replicating all steps present in `csv_to_hdf5.py` to make pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.copy()\n",
    "test2 = test.copy()\n",
    "meta2 = meta.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code proves that all the taxi ids in test set are also present in training set. We don't have any new taxi id in test data. But there are **20 origin calls** that are not in training data but are in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=0\n",
    "for l in test.TAXI_ID.unique():\n",
    "    if l not in train.TAXI_ID.unique():\n",
    "        s = s+1\n",
    "p=0\n",
    "g = []\n",
    "for l in test.ORIGIN_CALL.unique():\n",
    "    if l not in train.ORIGIN_CALL.unique():\n",
    "        p = p+1\n",
    "        g.append(l)\n",
    "s,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g` as saved above has list of origin_calls that are in test but not in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-calculated below on train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mean = 41.15731\n",
    "lat_std = 0.074120656\n",
    "long_mean = -8.6161413\n",
    "long_std = 0.057200309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a new feature with only first 5 and last 5 coordinates. In cases where we don't have enough coordinates, we do `edge` padding (i.e. just adding first and last values in leftmost and rightmost sides respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_stop_inputs(k, train, test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function used to take first k and last k coordinates from given polyline.\n",
    "    For train - if total coordinates < 2 then return none, if > 2*k then return first k and last k\n",
    "    and if > 2 but < 2*k then do edge padding and return.\n",
    "    \n",
    "    \"\"\"\n",
    "    result_train = []\n",
    "    result_test = []\n",
    "    \n",
    "    for l in train[['LONGITUDE','LATITUDE']].iterrows():\n",
    "        if len(l[1][0]) < 2 or len(l[1][1]) < 2:\n",
    "            result_train.append(np.nan)\n",
    "        elif len(l[1][0][:-1]) >= 2*k:\n",
    "            result_train.append(np.concatenate([l[1][0][0:k],l[1][0][-k:],\n",
    "                                          l[1][1][0:k],l[1][1][-k:]]).flatten())\n",
    "        else:\n",
    "            l1 = np.lib.pad(l[1][0][:-1], (0,4*k-len(l[1][0][:-1])), mode='edge')\n",
    "            l2 = np.lib.pad(l[1][1][:-1], (0,4*k-len(l[1][1][:-1])), mode='edge')\n",
    "            result_train.append(np.concatenate([l1[0:k],l1[-k:],l2[0:k],l2[-k:]]).flatten())\n",
    "\n",
    "    for l in test[['LONGITUDE','LATITUDE']].iterrows(): \n",
    "        if len(l[1][0]) < 1 or len(l[1][1]) < 1:\n",
    "            result_test.append(np.nan)\n",
    "        elif len(l[1][0]) >= 2*k:\n",
    "            result_test.append(np.concatenate([l[1][0][0:k],l[1][0][-k:],l[1][1][0:k],l[1][1][-k:]]).flatten())\n",
    "        else:\n",
    "            l1 = np.lib.pad(l[1][0], (0,4*k-len(l[1][0])), mode='edge')\n",
    "            l2 = np.lib.pad(l[1][1], (0,4*k-len(l[1][1])), mode='edge')\n",
    "            result_test.append(np.concatenate([l1[0:k],l1[-k:],l2[0:k],l2[-k:]]).flatten())\n",
    "    \n",
    "    return pd.Series(result_train), pd.Series(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def feature_ext(train, test):   \n",
    "    \"\"\"\n",
    "    Function used to extract model features.\n",
    "    • extract lat/long from polyline\n",
    "    • lat/long normalized using pre-calculated mean and std\n",
    "    • origin_call and taxi_id converted to continuous numbers to get embeddings later\n",
    "    • categories convert to cat codes\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # origin_call -- train\n",
    "    train.ORIGIN_CALL.fillna(value = -1, inplace=True)\n",
    "    id_uniq = train.ORIGIN_CALL.unique()\n",
    "    \n",
    "    origin_call_dict = {o:i for i,o in enumerate(id_uniq)}\n",
    "\n",
    "    train['ORIGIN_CALL'] = train.ORIGIN_CALL.apply(lambda x: 0 if x == -1 \n",
    "                                                   or x == '' else origin_call_dict[x])\n",
    "    \n",
    "    \n",
    "    # origin_call --test (g list of ids in test not in train)\n",
    "    test.ORIGIN_CALL.fillna(value = -1, inplace=True)\n",
    "\n",
    "    test['ORIGIN_CALL'] = test.ORIGIN_CALL.apply(lambda x: 0 if x == -1 or x == '' \n",
    "                                              else (-2 if x in g[1:] else origin_call_dict[x]))\n",
    "    \n",
    "    \n",
    "    # origin_stand\n",
    "    train['ORIGIN_STAND']= pd.Series([0 if pd.isnull(x) or x=='' else int(x) for x in train[\"ORIGIN_STAND\"]])\n",
    "    test['ORIGIN_STAND']= pd.Series([0 if pd.isnull(x) or x=='' else int(x) for x in test[\"ORIGIN_STAND\"]])\n",
    "    \n",
    "    # taxi_id\n",
    "    id_uniq = train.TAXI_ID.unique()\n",
    "    taxi_id_dict = {o:i for i,o in enumerate(id_uniq)}\n",
    "    train['TAXI_ID'] = train.TAXI_ID.apply(lambda x: taxi_id_dict[x])\n",
    "    test['TAXI_ID'] = test.TAXI_ID.apply(lambda x: taxi_id_dict[x])\n",
    "\n",
    "    # day_type and call_type\n",
    "    # want 0 for A, 1 for B and 2 for C\n",
    "    train['DAY_TYPE'] = pd.Series([ord(x[0]) - ord('A') for x in train['DAY_TYPE']])\n",
    "    train['CALL_TYPE'] = pd.Series([ord(x[0]) - ord('A') for x in train['CALL_TYPE']])\n",
    "    test['DAY_TYPE'] = pd.Series([ord(x[0]) - ord('A') for x in test['DAY_TYPE']])\n",
    "    test['CALL_TYPE'] = pd.Series([ord(x[0]) - ord('A') for x in test['CALL_TYPE']])\n",
    "    \n",
    "    \n",
    "    # polyline\n",
    "    polyline1 = pd.Series([ast.literal_eval(x) for x in train['POLYLINE']])\n",
    "    polyline2 = pd.Series([ast.literal_eval(x) for x in test['POLYLINE']])\n",
    "    \n",
    "    # latitude and longitude\n",
    "    train['LATITUDE'] = pd.Series([np.array([point[1] for point in poly],dtype=np.float32) for poly in polyline1])\n",
    "    train['LONGITUDE'] = pd.Series([np.array([point[0] for point in poly],dtype=np.float32) for poly in polyline1])\n",
    "    \n",
    "    test['LATITUDE'] = pd.Series([np.array([point[1] for point in poly],dtype=np.float32) for poly in polyline2])\n",
    "    test['LONGITUDE'] = pd.Series([np.array([point[0] for point in poly],dtype=np.float32) for poly in polyline2])\n",
    "    \n",
    "    # target variable i.e. last coordinates\n",
    "    train['TARGET'] = pd.Series([[l[1][0][-1], l[1][1][-1]] if len(l[1][0]) > 1 \n",
    "                                else np.nan for l in train[['LONGITUDE','LATITUDE']].iterrows()])\n",
    "\n",
    "    # normalized lat long. We need normalized data for neural nets and this is like cont. variable (not cat)\n",
    "    train['LATITUDE'] = pd.Series([(t-lat_mean)/lat_std for t in train['LATITUDE']])\n",
    "    test['LATITUDE'] = pd.Series([(t-lat_mean)/lat_std for t in test['LATITUDE']])\n",
    "    \n",
    "    train['LONGITUDE'] = pd.Series([(t-long_mean)/long_std for t in train['LONGITUDE']])\n",
    "    test['LONGITUDE'] = pd.Series([(t-long_mean)/long_std for t in test['LONGITUDE']])\n",
    "\n",
    "    # first 5 and last 5 features\n",
    "    train['COORD_FEATURES'], test['COORD_FEATURES'] = start_stop_inputs(5, train, test)\n",
    "    \n",
    "    # day of week variable\n",
    "    train['DAY_OF_WEEK'] = pd.Series([datetime.datetime.fromtimestamp(t).weekday() for t in train['TIMESTAMP']])\n",
    "    test['DAY_OF_WEEK'] = pd.Series([datetime.datetime.fromtimestamp(t).weekday() for t in test['TIMESTAMP']])\n",
    "\n",
    "    # hour of day\n",
    "    train['HOUR'] = pd.Series([datetime.datetime.fromtimestamp(t).hour for t in train['TIMESTAMP']])\n",
    "    test['HOUR'] = pd.Series([datetime.datetime.fromtimestamp(t).hour for t in test['TIMESTAMP']])\n",
    "\n",
    "    # week of year\n",
    "    train['WEEK_OF_YEAR'] = pd.Series([datetime.datetime.fromtimestamp(t).isocalendar()[1] for t in train['TIMESTAMP']])\n",
    "    test['WEEK_OF_YEAR'] = pd.Series([datetime.datetime.fromtimestamp(t).isocalendar()[1] for t in test['TIMESTAMP']])\n",
    "\n",
    "        \n",
    "    #data = data.dropna()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, test2 = train.copy(), test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, test2 = feature_ext(train2, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>COORD_FEATURES</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>WEEK_OF_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "      <td>[-0.21451, -0.214974, -0.199688, -0.182087, -0...</td>\n",
       "      <td>[-0.0437321, -0.0412145, -0.0731591, -0.105104...</td>\n",
       "      <td>[-8.63084, 41.1545]</td>\n",
       "      <td>[-0.0437321, -0.0412145, -0.0731591, -0.105104...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372637303620000596</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1372637303</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n",
       "      <td>[0.0339161, 0.0345337, 0.0378275, 0.0429227, 0...</td>\n",
       "      <td>[-0.414429, -0.423249, -0.455494, -0.494991, -...</td>\n",
       "      <td>[-8.66574, 41.1707]</td>\n",
       "      <td>[-0.414429, -0.423249, -0.455494, -0.494991, -...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372636951620000320</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1372636951</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n",
       "      <td>[-0.228715, -0.228818, -0.229796, -0.228561, -...</td>\n",
       "      <td>[0.0555529, 0.048317, 0.0336785, 0.0239251, 0....</td>\n",
       "      <td>[-8.61597, 41.1405]</td>\n",
       "      <td>[0.0555529, 0.048317, 0.0336785, 0.0239251, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372636854620000520</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1372636854</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n",
       "      <td>[-0.0723098, -0.0724127, -0.0725671, -0.072206...</td>\n",
       "      <td>[0.724872, 0.724405, 0.724572, 0.725189, 0.724...</td>\n",
       "      <td>[-8.608, 41.1429]</td>\n",
       "      <td>[0.724872, 0.724405, 0.724572, 0.725189, 0.724...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372637091620000337</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1372637091</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n",
       "      <td>[0.312708, 0.313068, 0.306789, 0.291092, 0.285...</td>\n",
       "      <td>[-0.5219, -0.521117, -0.522834, -0.536055, -0....</td>\n",
       "      <td>[-8.68727, 41.1781]</td>\n",
       "      <td>[-0.5219, -0.521117, -0.522834, -0.536055, -0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID  CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND  TAXI_ID  \\\n",
       "0  1372636858620000589          2            0             0        0   \n",
       "1  1372637303620000596          1            0             7        1   \n",
       "2  1372636951620000320          2            0             0        2   \n",
       "3  1372636854620000520          2            0             0        3   \n",
       "4  1372637091620000337          2            0             0        4   \n",
       "\n",
       "    TIMESTAMP  DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858         0         False   \n",
       "1  1372637303         0         False   \n",
       "2  1372636951         0         False   \n",
       "3  1372636854         0         False   \n",
       "4  1372637091         0         False   \n",
       "\n",
       "                                            POLYLINE  \\\n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...   \n",
       "1  [[-8.639847,41.159826],[-8.640351,41.159871],[...   \n",
       "2  [[-8.612964,41.140359],[-8.613378,41.14035],[-...   \n",
       "3  [[-8.574678,41.151951],[-8.574705,41.151942],[...   \n",
       "4  [[-8.645994,41.18049],[-8.645949,41.180517],[-...   \n",
       "\n",
       "                                            LATITUDE  \\\n",
       "0  [-0.21451, -0.214974, -0.199688, -0.182087, -0...   \n",
       "1  [0.0339161, 0.0345337, 0.0378275, 0.0429227, 0...   \n",
       "2  [-0.228715, -0.228818, -0.229796, -0.228561, -...   \n",
       "3  [-0.0723098, -0.0724127, -0.0725671, -0.072206...   \n",
       "4  [0.312708, 0.313068, 0.306789, 0.291092, 0.285...   \n",
       "\n",
       "                                           LONGITUDE               TARGET  \\\n",
       "0  [-0.0437321, -0.0412145, -0.0731591, -0.105104...  [-8.63084, 41.1545]   \n",
       "1  [-0.414429, -0.423249, -0.455494, -0.494991, -...  [-8.66574, 41.1707]   \n",
       "2  [0.0555529, 0.048317, 0.0336785, 0.0239251, 0....  [-8.61597, 41.1405]   \n",
       "3  [0.724872, 0.724405, 0.724572, 0.725189, 0.724...    [-8.608, 41.1429]   \n",
       "4  [-0.5219, -0.521117, -0.522834, -0.536055, -0....  [-8.68727, 41.1781]   \n",
       "\n",
       "                                      COORD_FEATURES  DAY_OF_WEEK  HOUR  \\\n",
       "0  [-0.0437321, -0.0412145, -0.0731591, -0.105104...            0     0   \n",
       "1  [-0.414429, -0.423249, -0.455494, -0.494991, -...            0     0   \n",
       "2  [0.0555529, 0.048317, 0.0336785, 0.0239251, 0....            0     0   \n",
       "3  [0.724872, 0.724405, 0.724572, 0.725189, 0.724...            0     0   \n",
       "4  [-0.5219, -0.521117, -0.522834, -0.536055, -0....            0     0   \n",
       "\n",
       "   WEEK_OF_YEAR  \n",
       "0            27  \n",
       "1            27  \n",
       "2            27  \n",
       "3            27  \n",
       "4            27  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>COORD_FEATURES</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>WEEK_OF_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>1408039037</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.585676,41.148522],[-8.585712,41.148639],[...</td>\n",
       "      <td>[-0.118578, -0.116982, -0.1141, -0.113122, -0....</td>\n",
       "      <td>[0.532604, 0.531971, 0.532454, 0.531671, 0.527...</td>\n",
       "      <td>[0.532604, 0.531971, 0.532454, 0.531671, 0.527...</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>306</td>\n",
       "      <td>1408038611</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.610876,41.14557],[-8.610858,41.145579],[-...</td>\n",
       "      <td>[-0.158413, -0.158258, -0.155736, -0.150024, -...</td>\n",
       "      <td>[0.0920491, 0.0923659, 0.0915823, 0.0996017, 0...</td>\n",
       "      <td>[0.0920491, 0.0923659, 0.0915823, 0.0996017, 0...</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>260</td>\n",
       "      <td>1408038568</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.585739,41.148558],[-8.58573,41.148828],[-...</td>\n",
       "      <td>[-0.118063, -0.11446, -0.112505, -0.111887, -0...</td>\n",
       "      <td>[0.531504, 0.531671, 0.531821, 0.5219, 0.52490...</td>\n",
       "      <td>[0.531504, 0.531671, 0.531821, 0.5219, 0.52490...</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>1408039090</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.613963,41.141169],[-8.614125,41.141124],[...</td>\n",
       "      <td>[-0.217753, -0.21837, -0.221047, -0.222488, -0...</td>\n",
       "      <td>[0.0380801, 0.0352457, 0.0184065, 0.0151053, 0...</td>\n",
       "      <td>[0.0380801, 0.0352457, 0.0184065, 0.0151053, 0...</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>1408039177</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.619903,41.148036],[-8.619894,41.148036]]</td>\n",
       "      <td>[-0.125114, -0.125114]</td>\n",
       "      <td>[-0.0657565, -0.0656064]</td>\n",
       "      <td>[-0.0657565, -0.0656064, -0.0656064, -0.065606...</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TRIP_ID  CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND  TAXI_ID   TIMESTAMP  \\\n",
       "0      T1          1            0            15      150  1408039037   \n",
       "1      T2          1            0            57      306  1408038611   \n",
       "2      T3          1            0            15      260  1408038568   \n",
       "3      T4          1            0            53       40  1408039090   \n",
       "4      T5          1            0            18       33  1408039177   \n",
       "\n",
       "   DAY_TYPE  MISSING_DATA                                           POLYLINE  \\\n",
       "0         0         False  [[-8.585676,41.148522],[-8.585712,41.148639],[...   \n",
       "1         0         False  [[-8.610876,41.14557],[-8.610858,41.145579],[-...   \n",
       "2         0         False  [[-8.585739,41.148558],[-8.58573,41.148828],[-...   \n",
       "3         0         False  [[-8.613963,41.141169],[-8.614125,41.141124],[...   \n",
       "4         0         False      [[-8.619903,41.148036],[-8.619894,41.148036]]   \n",
       "\n",
       "                                            LATITUDE  \\\n",
       "0  [-0.118578, -0.116982, -0.1141, -0.113122, -0....   \n",
       "1  [-0.158413, -0.158258, -0.155736, -0.150024, -...   \n",
       "2  [-0.118063, -0.11446, -0.112505, -0.111887, -0...   \n",
       "3  [-0.217753, -0.21837, -0.221047, -0.222488, -0...   \n",
       "4                             [-0.125114, -0.125114]   \n",
       "\n",
       "                                           LONGITUDE  \\\n",
       "0  [0.532604, 0.531971, 0.532454, 0.531671, 0.527...   \n",
       "1  [0.0920491, 0.0923659, 0.0915823, 0.0996017, 0...   \n",
       "2  [0.531504, 0.531671, 0.531821, 0.5219, 0.52490...   \n",
       "3  [0.0380801, 0.0352457, 0.0184065, 0.0151053, 0...   \n",
       "4                           [-0.0657565, -0.0656064]   \n",
       "\n",
       "                                      COORD_FEATURES  DAY_OF_WEEK  HOUR  \\\n",
       "0  [0.532604, 0.531971, 0.532454, 0.531671, 0.527...            3    17   \n",
       "1  [0.0920491, 0.0923659, 0.0915823, 0.0996017, 0...            3    17   \n",
       "2  [0.531504, 0.531671, 0.531821, 0.5219, 0.52490...            3    17   \n",
       "3  [0.0380801, 0.0352457, 0.0184065, 0.0151053, 0...            3    17   \n",
       "4  [-0.0657565, -0.0656064, -0.0656064, -0.065606...            3    17   \n",
       "\n",
       "   WEEK_OF_YEAR  \n",
       "0            33  \n",
       "1            33  \n",
       "2            33  \n",
       "3            33  \n",
       "4            33  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving transformed files as `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.to_pickle(path+'train2pickle')\n",
    "test2.to_pickle(path+'test2pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading train and test. **Can directly run this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.read_pickle('train2pickle')\n",
    "test2 = pd.read_pickle('test2pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Meanshift clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from itertools import cycle\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.17067, -8.66574)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how to index and subset lat and long (lat is in 1 and long in 0th)\n",
    "train2.TARGET[:10][1][1], train2.TARGET[:10][1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have polyline data for all the training data. For those, we have `nan` in `TARGET` variable and those won't contribute in centroid calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating arrival point list\n",
      "Doing clustering\n",
      "0.0215466177411\n",
      "Clusters shape:  (3422, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating arrival point list\")\n",
    "dests = []\n",
    "\n",
    "# for l in train2[['LONGITUDE','LATITUDE']].iterrows():\n",
    "#     if len(l[1][0]) == 0 or len(l[1][1]) == 0: continue\n",
    "#     dests.append((l[1][1][-1],  l[1][0][-1])) # latitude, longitude\n",
    "\n",
    "target = train2.TARGET.fillna(0)    \n",
    "for l in target:\n",
    "    if l == 0 : continue\n",
    "    dests.append((l[1],  l[0])) # latitude, longitude\n",
    "    \n",
    "pts = np.array(dests)\n",
    "\n",
    "with open(os.path.join(path, \"arrivals.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pts, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Doing clustering\")\n",
    "bw = estimate_bandwidth(pts, quantile=.1, n_samples=1000)\n",
    "print(bw)\n",
    "bw = 0.001 \n",
    "\n",
    "ms = MeanShift(bandwidth=bw, bin_seeding=True, min_bin_freq=5)\n",
    "ms.fit(pts)\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "print(\"Clusters shape: \", cluster_centers.shape)\n",
    "\n",
    "with open(os.path.join(path, \"arrival-clusters.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(cluster_centers, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried above code for --  \n",
    "1. scaled coordinates. It doesn't give even close number of clusters. got **22644 clusters**. Not quite right. \n",
    "2. unscaled coordinates. Got **3422 clusters**. Winner had 3392 clusters. CLOSE and will keep it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Making Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As given in winner's solution, we are taking validation set as last year's data with same timestamp as of test data timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuts of the test set minus 1 year\n",
    "cuts = [\n",
    "    1376503200, # 2013-08-14 18:00\n",
    "    1380616200, # 2013-10-01 08:30\n",
    "    1381167900, # 2013-10-07 17:45\n",
    "    1383364800, # 2013-11-02 04:00\n",
    "    1387722600  # 2013-12-22 14:30\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indices = []\n",
    "index = 0\n",
    "for index, row in train2.iterrows():\n",
    "    time = row['TIMESTAMP']\n",
    "    latitude = row['LATITUDE']\n",
    "    for ts in cuts:\n",
    "        if time <= ts and time + 15 * (len(latitude) - 1) >= ts:\n",
    "            val_indices.append(index)\n",
    "            break\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving and reloading validation indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"val_idx.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(val_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"val_idx.pkl\"), \"rb\") as f:\n",
    "    val_indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = train2.iloc[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have right timestamps in validation set or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{datetime.date(2013, 8, 14),\n",
       " datetime.date(2013, 10, 1),\n",
       " datetime.date(2013, 10, 7),\n",
       " datetime.date(2013, 11, 2),\n",
       " datetime.date(2013, 12, 22)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt = []\n",
    "for d in X_valid['TIMESTAMP']:\n",
    "    lt.append(datetime.date.fromtimestamp(d))\n",
    "    \n",
    "set(lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup. above dates match with our dates of test set. Just 1 previous year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train2.drop(train2.index[[val_indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 1710366)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_valid), len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set length is similar as test set length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"arrival-clusters.pkl\"), \"rb\") as f:\n",
    "    cluster_centers2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3422"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_centers2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving `X_train` and `X_valid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(path + 'X_train.pkl')\n",
    "X_valid.to_pickle(path + 'X_valid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading validation and training files. Next time can directly read from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('X_train.pkl')\n",
    "X_valid = pd.read_pickle('X_valid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Defining Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They used `equirectangular distance` as loss function which performed better than original kaggle metric (`Haversine distance`). Actually equirectangular distance is just a simpler approximation to haversine distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will keep long in 0 and lat in 1 as there in target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearth = 6371\n",
    "deg2rad = 3.141592653589793 / 180.\n",
    "\n",
    "def erdist(a, b):\n",
    "    lat1 = a[:, 1] * deg2rad\n",
    "    lon1 = a[:, 0] * deg2rad\n",
    "    lat2 = b[:, 1] * deg2rad\n",
    "    lon2 = b[:, 0] * deg2rad\n",
    "    x = (lon2-lon1) * torch.cos((lat1+lat2)/2)\n",
    "    y = (lat2-lat1)\n",
    "    return torch.sqrt(x**2 + y**2) * rearth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer is basically telling probability of observation being in ith cluster. So it will have as many outputs as number of clusters. (3422). It includes training for `probabilities`  \n",
    "\n",
    "Now sum of all the `probabilities` should be 1. Therefore we will use `softmax` over a trained linear layer\n",
    "\n",
    "\n",
    "On these 3422 output from hidden, we take weighted avg which will just be sum of multiplied probability into cluster center (i.e. sum of 3422 elements). It includes no traning, just using previous probabilities and calculating weighted avg.   \n",
    "\n",
    "Think of this operation as a linear layer with `weights = centers (i.e. fixed) and input = probabilities`\n",
    "\n",
    "\n",
    "\n",
    "Layers to add:  \n",
    "1. Pass input data to a **dense layer** which has 500 outputs and those **500 to relu**. \n",
    "1. Pass coming data to a **linear layer** which has 3422 outputs\n",
    "2. These 3422 outputs passed to a **softmax** (which will give p)\n",
    "3. These 3422 p's will go to **linear layer** which **does not** update weights.  just multiply p and c . \n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing cardinality for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ORIGIN_CALL', 'TAXI_ID', 'ORIGIN_STAND', 'HOUR', 'DAY_OF_WEEK', 'WEEK_OF_YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGIN_CALL -- Possible values 57106 || Embedding size 10\n",
      "TAXI_ID -- Possible values 448 || Embedding size 10\n",
      "ORIGIN_STAND -- Possible values 64 || Embedding size 10\n",
      "HOUR -- Possible values 24 || Embedding size 10\n",
      "DAY_OF_WEEK -- Possible values 7 || Embedding size 10\n",
      "WEEK_OF_YEAR -- Possible values 52 || Embedding size 10\n"
     ]
    }
   ],
   "source": [
    "for i in cols:\n",
    "    print(i, '--', 'Possible values', len(train2[i].unique()) ,'||', 'Embedding size', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They took embedding of size 10 for all the above features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57106, 10), (448, 10), (64, 10), (24, 10), (7, 10), (52, 10)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs = []\n",
    "for i in cols:\n",
    "    emb_szs.append((len(train2[i].unique()), 10))\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['ORIGIN_CALL', 'TAXI_ID', 'ORIGIN_STAND', 'HOUR', 'DAY_OF_WEEK', 'WEEK_OF_YEAR']\n",
    "#cont_vars = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>COORD_FEATURES</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>WEEK_OF_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "      <td>[-0.21451, -0.214974, -0.199688, -0.182087, -0...</td>\n",
       "      <td>[-0.0437321, -0.0412145, -0.0731591, -0.105104...</td>\n",
       "      <td>[-8.63084, 41.1545]</td>\n",
       "      <td>[-0.0437321, -0.0412145, -0.0731591, -0.105104...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID  CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND  TAXI_ID  \\\n",
       "0  1372636858620000589          2            0             0        0   \n",
       "\n",
       "    TIMESTAMP  DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858         0         False   \n",
       "\n",
       "                                            POLYLINE  \\\n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...   \n",
       "\n",
       "                                            LATITUDE  \\\n",
       "0  [-0.21451, -0.214974, -0.199688, -0.182087, -0...   \n",
       "\n",
       "                                           LONGITUDE               TARGET  \\\n",
       "0  [-0.0437321, -0.0412145, -0.0731591, -0.105104...  [-8.63084, 41.1545]   \n",
       "\n",
       "                                      COORD_FEATURES  DAY_OF_WEEK  HOUR  \\\n",
       "0  [-0.0437321, -0.0412145, -0.0731591, -0.105104...            0     0   \n",
       "\n",
       "   WEEK_OF_YEAR  \n",
       "0            27  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NA with 0s\n",
    "# k = 5\n",
    "\n",
    "X_train.COORD_FEATURES.fillna(0, inplace = True)\n",
    "X_valid.COORD_FEATURES.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    \n",
    "    cols = ['ORIGIN_CALL', 'TAXI_ID', 'ORIGIN_STAND', 'HOUR', 'DAY_OF_WEEK', 'WEEK_OF_YEAR','MISSING_DATA']\n",
    "    \n",
    "    df_feat = df[cols]\n",
    "    for i in range(10):\n",
    "        df_feat[f'long_{i}'] = df.COORD_FEATURES.apply(lambda x: 0 if type(x) == int else x[i]) # creating 10 cols for long\n",
    "        df_feat[f'lat_{i}'] = df.COORD_FEATURES.apply(lambda x: 0 if type(x) == int else x[i+10]) # 10 cols for lat\n",
    "\n",
    "    df_target = df['TARGET']\n",
    "               \n",
    "    return df_feat, df_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.COORD_FEATURES.fillna(0, inplace = True)\n",
    "df, y = get_features(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat, train_target = get_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_feat, valid_target = get_features(X_valid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining nn for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centers of cluster lats and longs. Needed for last nn layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_lats = []\n",
    "\n",
    "for i in cluster_centers2:\n",
    "    cluster_lats.append(i[0])\n",
    "    \n",
    "cluster_longs = []\n",
    "\n",
    "for i in cluster_centers2:\n",
    "    cluster_longs.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_lats = np.array(cluster_lats)[:,None]\n",
    "cluster_longs = np.array(cluster_longs)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed input nn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>WEEK_OF_YEAR</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>long_0</th>\n",
       "      <th>lat_0</th>\n",
       "      <th>long_1</th>\n",
       "      <th>...</th>\n",
       "      <th>long_5</th>\n",
       "      <th>lat_5</th>\n",
       "      <th>long_6</th>\n",
       "      <th>lat_6</th>\n",
       "      <th>long_7</th>\n",
       "      <th>lat_7</th>\n",
       "      <th>long_8</th>\n",
       "      <th>lat_8</th>\n",
       "      <th>long_9</th>\n",
       "      <th>lat_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.043732</td>\n",
       "      <td>-0.214510</td>\n",
       "      <td>-0.041215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262276</td>\n",
       "      <td>-0.038085</td>\n",
       "      <td>-0.256774</td>\n",
       "      <td>-0.037828</td>\n",
       "      <td>-0.256774</td>\n",
       "      <td>-0.037725</td>\n",
       "      <td>-0.256774</td>\n",
       "      <td>-0.037930</td>\n",
       "      <td>-0.256940</td>\n",
       "      <td>-0.038085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.414429</td>\n",
       "      <td>0.033916</td>\n",
       "      <td>-0.423249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.934763</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>-0.908337</td>\n",
       "      <td>0.157538</td>\n",
       "      <td>-0.882844</td>\n",
       "      <td>0.171279</td>\n",
       "      <td>-0.867572</td>\n",
       "      <td>0.179771</td>\n",
       "      <td>-0.867105</td>\n",
       "      <td>0.180234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ORIGIN_CALL  TAXI_ID  ORIGIN_STAND  HOUR  DAY_OF_WEEK  WEEK_OF_YEAR  \\\n",
       "0            0        0             0     0            0            27   \n",
       "1            0        1             7     0            0            27   \n",
       "\n",
       "   MISSING_DATA    long_0     lat_0    long_1    ...       long_5     lat_5  \\\n",
       "0         False -0.043732 -0.214510 -0.041215    ...    -0.262276 -0.038085   \n",
       "1         False -0.414429  0.033916 -0.423249    ...    -0.934763  0.143693   \n",
       "\n",
       "     long_6     lat_6    long_7     lat_7    long_8     lat_8    long_9  \\\n",
       "0 -0.256774 -0.037828 -0.256774 -0.037725 -0.256774 -0.037930 -0.256940   \n",
       "1 -0.908337  0.157538 -0.882844  0.171279 -0.867572  0.179771 -0.867105   \n",
       "\n",
       "      lat_9  \n",
       "0 -0.038085  \n",
       "1  0.180234  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_init(x):\n",
    "    x = x.weight.data\n",
    "    sc = 2/(x.size(1)+1)\n",
    "    x.uniform_(-sc,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they didn't do batchnormm in paper            \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n",
    "                 y_range=None, use_bn=False):\n",
    "        super().__init__()\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n",
    "        for emb in self.embs: emb_init(emb)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "\n",
    "        szs = [n_emb+n_cont] + szs        \n",
    "        self.lin1 = nn.Linear(len(szs), 500)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(500, 3422)\n",
    "        self.soft = nn.Softmax() # got probabilities now\n",
    "         \n",
    "        for o in [self.lin1, self.lin2] : kaiming_normal(o.weight.data)\n",
    "            \n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x2 = self.bn(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        \n",
    "        out = self.lin1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.soft()\n",
    "        outlat = out*cluster_lats\n",
    "        outlong = out*cluster_longs\n",
    "            \n",
    "        return [outlong, outlat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customizing `fast.ai` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.dataset import *\n",
    "from fastai.learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnarDatasetCustom(Dataset):\n",
    "    def __init__(self, cats, conts, y):\n",
    "        n = len(cats[0]) if cats else len(conts[0])\n",
    "        self.cats = np.stack(cats, 1).astype(np.int64) if cats else np.zeros((n,1))\n",
    "        self.conts = np.stack(conts, 1).astype(np.float32) if conts else np.zeros((n,1))\n",
    "        self.y = np.zeros((n,1)) if y is None else y\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.y[idx]]\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frames(cls, df_cat, df_cont, y=None):\n",
    "        cat_cols = [c.values for n,c in df_cat.items()]\n",
    "        cont_cols = [c.values for n,c in df_cont.items()]\n",
    "        return cls(cat_cols, cont_cols, y)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, df, cat_flds, y=None):\n",
    "        return cls.from_data_frames(df[cat_flds], df.drop(cat_flds, axis=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnarModelDataCustom(ModelData):\n",
    "    def __init__(self, path, trn_ds, val_ds, bs, test_ds=None):\n",
    "        test_dl = DataLoader(test_ds, bs, shuffle=False, num_workers=1) if test_ds is not None else None\n",
    "        super().__init__(path, DataLoader(trn_ds, bs, shuffle=True, num_workers=1),\n",
    "            DataLoader(val_ds, bs*2, shuffle=False, num_workers=1), test_dl)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frames(cls, path, trn_df, val_df, trn_y, val_y, cat_flds, bs, test_df=None):\n",
    "        test_ds = ColumnarDataset.from_data_frame(test_df, cat_flds) if test_df is not None else None\n",
    "        return cls(path, ColumnarDatasetCustom.from_data_frame(trn_df, cat_flds, trn_y),\n",
    "                    ColumnarDatasetCustom.from_data_frame(val_df, cat_flds, val_y), bs, test_ds=test_ds)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, path, val_idxs, df, y, cat_flds, bs, test_df=None):\n",
    "        ((val_df, trn_df), (val_y, trn_y)) = split_by_idx(val_idxs, df, y)\n",
    "        return cls.from_data_frames(path, trn_df, val_df, trn_y, val_y, cat_flds, bs, test_df=test_df)\n",
    "    \n",
    "    def get_learner_custom(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n",
    "                y_range=None, use_bn=False, **kwargs):\n",
    "        model = Net(emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn)\n",
    "        return StructuredLearnerCustom(self, StructuredModelCustom(to_gpu(model)), opt_fn=optim.SGD, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredLearnerCustom(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "        #self.crit = F.mse_loss\n",
    "        self.crit = erdist\n",
    "\n",
    "class StructuredModelCustom(BasicModel):\n",
    "    def get_layer_groups(self):\n",
    "        m=self.model\n",
    "        return [m.embs, children(m.lin1),m.lin2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitting model using above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelDataCustom.from_data_frame(path, val_indices, df, y, cat_flds=cat_vars, bs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = md.get_learner_custom(emb_szs, len(df.columns)-len(cat_vars),\n",
    "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (embs): ModuleList (\n",
       "    (0): Embedding(57106, 10)\n",
       "    (1): Embedding(448, 10)\n",
       "    (2): Embedding(64, 10)\n",
       "    (3): Embedding(24, 10)\n",
       "    (4): Embedding(7, 10)\n",
       "    (5): Embedding(52, 10)\n",
       "  )\n",
       "  (lin1): Linear (3 -> 500)\n",
       "  (relu): ReLU ()\n",
       "  (lin2): Linear (500 -> 3422)\n",
       "  (soft): Softmax ()\n",
       "  (bn): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c48cf1105a84b549dcbdec5bced6715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8552 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip argument #71 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-3b1c17f5ae8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m---> 89\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/dataloader.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/dataloader.py\u001b[0m in \u001b[0;36mnp_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch must contain numbers, dicts or lists; found {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch must contain numbers, dicts or lists; found {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/dataloader.py\u001b[0m in \u001b[0;36mnp_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch must contain numbers, dicts or lists; found {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #71 must support iteration"
     ]
    }
   ],
   "source": [
    "m.fit(lr, 3, metrics=[erdist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class taxiDataset(Dataset):\n",
    "    def __init__(self, cats, conts, y):\n",
    "        n = len(cats[0]) if cats else len(conts[0])\n",
    "        self.cats = np.stack(cats, 1).astype(np.int64) if cats else np.zeros((n,1))\n",
    "        self.conts = np.stack(conts, 1).astype(np.float32) if conts else np.zeros((n,1))\n",
    "        self.y = np.zeros((n,1)) if y is None else y\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.y[idx]]\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frames(cls, df_cat, df_cont, y=None):\n",
    "        cat_cols = [c.values for n,c in df_cat.items()]\n",
    "        cont_cols = [c.values for n,c in df_cont.items()]\n",
    "        return cls(cat_cols, cont_cols, y)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, df, cat_flds, y=None):\n",
    "        return cls.from_data_frames(df[cat_flds], df.drop(cat_flds, axis=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ColumnarModelDataCustom' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-8ef7f8aaf106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Convert torch tensor to Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ColumnarModelDataCustom' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(md):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28*28)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `thnn.NLLLoss` not found.\n"
     ]
    }
   ],
   "source": [
    "thnn.NLLLoss??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_dist_f(input, target, weight=None, size_average=True, ignore_index=-100):\n",
    "    return _functions.thnn.erdist.apply(input, target, weight, size_average, ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class erdist(_WeightedLoss):\n",
    "  \n",
    "\n",
    "    def __init__(self, weight=None, size_average=True, ignore_index=-100):\n",
    "        super(erdist, self).__init__(weight, size_average)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        _assert_no_grad(target)\n",
    "        return F.er_dist_f(input, target, self.weight, self.size_average,\n",
    "                          self.ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = m\n",
    "loss=nn.NLLLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer=optim.SGD(net2.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(1):\n",
    "    losses=[]\n",
    "    dl = iter(md.trn_dl)\n",
    "    for t in range(len(dl)):\n",
    "        # Forward pass: compute predicted y and loss by passing x to the model.\n",
    "        xt, yt = next(dl)\n",
    "        y_pred = net2(V(xt))\n",
    "        l = loss(y_pred, V(yt))\n",
    "        losses.append(l)\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable weights of the model)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        l.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    val_dl = iter(md.val_dl)\n",
    "    val_scores = [score(*next(val_dl)) for i in range(len(val_dl))]\n",
    "    print(np.mean(val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
